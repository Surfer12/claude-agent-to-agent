# Ollama Configuration for Docker Container
# CPU-only mode (Metal GPU not available in Docker on macOS)

# Server configuration
host: "0.0.0.0:11434"

# Model storage location
home: "/ollama"

# Security settings
no_network: false  # Set to true for offline mode after models are cached

# CPU configuration for M4 Max
# Note: Docker containers cannot access Metal GPU on macOS
# For GPU acceleration, run Ollama natively on macOS

# Logging
log_level: "info"

# Model settings
# Default model to load (optional)
# default_model: "llama2:7b-q8_0"

# Performance tuning for CPU-only mode
# These settings optimize for CPU inference
cpu_threads: 8  # Adjust based on your M4 Max core count
cpu_memory: "16g"  # Reserve memory for CPU inference

# Security: disable model downloads from untrusted sources
# allow_downloads: false  # Uncomment to disable downloads

# Rate limiting (optional)
# rate_limit: 100  # requests per minute