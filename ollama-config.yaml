# =============================================================================
# Ollama Configuration for macOS M4 Max (48GB) - Docker Deployment
# =============================================================================
# This configuration file optimizes Ollama for secure, CPU-only inference
# inside a Docker container on Apple Silicon M4 Max hardware
# =============================================================================

# -----------------------------------------------------------------------------
# üîß Server Configuration
# -----------------------------------------------------------------------------
server:
  host: "0.0.0.0"
  port: 11434
  
  # Request timeout settings
  timeout:
    read: "5m"
    write: "5m"
    idle: "2m"
  
  # Maximum request size (for large prompts)
  max_request_size: "100MB"

# -----------------------------------------------------------------------------
# üíæ Model Management
# -----------------------------------------------------------------------------
models:
  # Model storage directory (mounted from host)
  path: "/ollama/models"
  
  # Maximum number of models to keep loaded simultaneously
  max_loaded: 2
  
  # Model loading timeout
  load_timeout: "10m"
  
  # Automatic model unloading after inactivity
  unload_after: "30m"
  
  # Model verification (checksum validation)
  verify_checksums: true

# -----------------------------------------------------------------------------
# üß† Memory Management (Optimized for 48GB M4 Max)
# -----------------------------------------------------------------------------
memory:
  # Use 70% of available container memory for models
  model_memory_fraction: 0.7
  
  # Reserve memory for system operations
  system_memory_reserve: "4GB"
  
  # Enable memory mapping for large models
  use_mmap: true
  
  # Memory pool settings
  pool:
    initial_size: "8GB"
    max_size: "24GB"

# -----------------------------------------------------------------------------
# üîÑ CPU Configuration (M4 Max has 14 cores: 10 performance + 4 efficiency)
# -----------------------------------------------------------------------------
cpu:
  # Number of threads for model inference
  num_threads: 8
  
  # Enable CPU optimizations
  optimizations:
    enable_avx: true
    enable_avx2: true
    enable_fma: true
    enable_f16c: true
  
  # CPU affinity (use performance cores)
  affinity: "0-7"

# -----------------------------------------------------------------------------
# üöÄ Performance Tuning
# -----------------------------------------------------------------------------
performance:
  # Batch processing settings
  batch:
    max_size: 32
    timeout: "100ms"
  
  # Context length limits
  context:
    max_length: 8192
    default_length: 4096
  
  # Parallel processing
  parallel:
    max_requests: 4
    max_sequences: 2
  
  # Flash attention (if supported)
  flash_attention: true
  
  # KV cache optimization
  kv_cache:
    type: "fp16"
    size_limit: "8GB"

# -----------------------------------------------------------------------------
# üîí Security Configuration
# -----------------------------------------------------------------------------
security:
  # CORS settings
  cors:
    enabled: true
    allowed_origins: ["http://localhost:*", "http://127.0.0.1:*"]
    allowed_methods: ["GET", "POST", "OPTIONS"]
    allowed_headers: ["Content-Type", "Authorization"]
  
  # API security
  api:
    # Rate limiting
    rate_limit:
      enabled: true
      requests_per_minute: 60
      burst: 10
    
    # Request size limits
    max_prompt_length: 32768
    max_response_length: 8192
  
  # Logging security
  logging:
    log_requests: false  # Don't log sensitive prompts
    log_responses: false # Don't log generated content
    log_level: "INFO"

# -----------------------------------------------------------------------------
# üìù Logging Configuration
# -----------------------------------------------------------------------------
logging:
  # Log file location
  file: "/ollama/logs/ollama.log"
  
  # Log rotation
  rotation:
    max_size: "100MB"
    max_files: 5
    compress: true
  
  # Log format
  format: "json"
  
  # Log levels by component
  levels:
    server: "INFO"
    model: "WARN"
    memory: "WARN"
    performance: "INFO"

# -----------------------------------------------------------------------------
# üåê Network Configuration
# -----------------------------------------------------------------------------
network:
  # Connection limits
  max_connections: 100
  
  # Keep-alive settings
  keep_alive:
    enabled: true
    timeout: "75s"
    interval: "15s"
  
  # Disable external network access (for offline mode)
  # offline_mode: true  # Uncomment after downloading models

# -----------------------------------------------------------------------------
# üîß Model-Specific Configurations
# -----------------------------------------------------------------------------
model_configs:
  # Configuration for Llama 2 7B models
  "llama2:7b*":
    context_length: 4096
    temperature: 0.7
    top_p: 0.9
    top_k: 40
    repeat_penalty: 1.1
    
  # Configuration for Code Llama models
  "codellama:*":
    context_length: 8192
    temperature: 0.1
    top_p: 0.95
    
  # Configuration for smaller models (3B and below)
  "*:3b*":
    context_length: 2048
    num_threads: 4

# -----------------------------------------------------------------------------
# üõ†Ô∏è Advanced Settings
# -----------------------------------------------------------------------------
advanced:
  # Experimental features
  experimental:
    enable_model_offloading: true
    enable_dynamic_batching: true
    enable_speculative_decoding: false
  
  # Debug settings (disable in production)
  debug:
    enabled: false
    profile_memory: false
    profile_cpu: false
    
  # Compatibility settings
  compatibility:
    strict_mode: false
    legacy_api: false