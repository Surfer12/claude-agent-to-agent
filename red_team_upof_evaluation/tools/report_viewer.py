"""
UPOF Red Team Report Viewer

This tool provides visualization and analysis capabilities for evaluation reports
generated by the UPOF red team framework.
"""

import json
import argparse
from datetime import datetime
from typing import Dict, List, Any
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import defaultdict

class UPOFReportViewer:
    """Viewer and analyzer for UPOF evaluation reports."""
    
    def __init__(self, report_path: str):
        self.report_path = report_path
        self.report_data = self._load_report()
        self.df_results = self._create_results_dataframe()
    
    def _load_report(self) -> Dict[str, Any]:
        """Load the evaluation report from JSON file."""
        with open(self.report_path, 'r') as f:
            return json.load(f)
    
    def _create_results_dataframe(self) -> pd.DataFrame:
        """Create a pandas DataFrame from the detailed results."""
        results = []
        for result in self.report_data['detailed_results']:
            # Flatten the nested structure for easier analysis
            flat_result = {
                'test_id': result['test_id'],
                'timestamp': result['timestamp'],
                'model_name': result['model_config']['model_name'],
                'provider': result['model_config']['provider'],
                'temperature': result['model_config']['temperature'],
                'template_id': result['prompt_template']['id'],
                'template_title': result['prompt_template']['title'],
                'template_category': result['prompt_template']['category'],
                'target_vulnerability': result['prompt_template']['target_vulnerability'],
                'template_severity': result['prompt_template']['severity_level'],
                'safety_assessment': result['safety_assessment'],
                'severity_score': result['severity_score'],
                'breadth_score': result['breadth_score'],
                'vulnerability_detected': result['vulnerability_detected'],
                'execution_time': result['execution_time'],
                'analysis_notes': result['analysis_notes']
            }
            results.append(flat_result)
        
        return pd.DataFrame(results)
    
    def print_summary(self):
        """Print a comprehensive summary of the evaluation results."""
        print("=" * 80)
        print("UPOF RED TEAM EVALUATION REPORT SUMMARY")
        print("=" * 80)
        
        # Basic statistics
        print(f"\nReport ID: {self.report_data['report_id']}")
        print(f"Generated: {self.report_data['timestamp']}")
        print(f"Total Tests: {self.report_data['total_tests']}")
        print(f"Models Tested: {', '.join(self.report_data['models_tested'])}")
        
        print(f"\nSAFETY OVERVIEW:")
        print(f"  Safe Responses: {self.report_data['safe_responses']} ({self.report_data['safe_responses']/self.report_data['total_tests']*100:.1f}%)")
        print(f"  Vulnerable Responses: {self.report_data['vulnerable_responses']} ({self.report_data['vulnerable_responses']/self.report_data['total_tests']*100:.1f}%)")
        print(f"  Error Responses: {self.report_data['error_responses']} ({self.report_data['error_responses']/self.report_data['total_tests']*100:.1f}%)")
        
        # Vulnerability breakdown
        print(f"\nVULNERABILITY BREAKDOWN:")
        for vuln_type, count in self.report_data['vulnerability_summary'].items():
            print(f"  {vuln_type.replace('_', ' ').title()}: {count} vulnerabilities")
        
        # Severity distribution
        print(f"\nSEVERITY DISTRIBUTION:")
        for severity, count in sorted(self.report_data['severity_distribution'].items()):
            severity_label = ["None", "Low", "Low-Medium", "Medium", "High", "Critical"][int(severity)]
            print(f"  Level {severity} ({severity_label}): {count} responses")
        
        # Model performance comparison
        if len(self.report_data['models_tested']) > 1:
            print(f"\nMODEL PERFORMANCE COMPARISON:")
            model_stats = self.df_results.groupby('model_name').agg({
                'vulnerability_detected': ['count', 'sum'],
                'severity_score': 'mean',
                'execution_time': 'mean'
            }).round(2)
            
            for model in self.df_results['model_name'].unique():
                model_data = self.df_results[self.df_results['model_name'] == model]
                total_tests = len(model_data)
                vulnerable_tests = model_data['vulnerability_detected'].sum()
                avg_severity = model_data['severity_score'].mean()
                avg_time = model_data['execution_time'].mean()
                
                print(f"  {model}:")
                print(f"    Vulnerability Rate: {vulnerable_tests}/{total_tests} ({vulnerable_tests/total_tests*100:.1f}%)")
                print(f"    Average Severity: {avg_severity:.2f}")
                print(f"    Average Response Time: {avg_time:.2f}s")
        
        # Template performance
        print(f"\nTEST TEMPLATE PERFORMANCE:")
        template_stats = self.df_results.groupby('template_id').agg({
            'vulnerability_detected': ['count', 'sum'],
            'severity_score': 'mean'
        }).round(2)
        
        for template_id in self.df_results['template_id'].unique():
            template_data = self.df_results[self.df_results['template_id'] == template_id]
            total_tests = len(template_data)
            vulnerable_tests = template_data['vulnerability_detected'].sum()
            avg_severity = template_data['severity_score'].mean()
            template_title = template_data['template_title'].iloc[0]
            
            print(f"  {template_id} ({template_title}):")
            print(f"    Vulnerability Rate: {vulnerable_tests}/{total_tests} ({vulnerable_tests/total_tests*100:.1f}%)")
            print(f"    Average Severity: {avg_severity:.2f}")
        
        # Recommendations
        print(f"\nRECOMMENDATIONS:")
        for i, rec in enumerate(self.report_data['recommendations'], 1):
            print(f"  {i}. {rec}")
        
        print("=" * 80)
    
    def create_visualizations(self, output_dir: str = "visualizations"):
        """Create comprehensive visualizations of the evaluation results."""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # Set style
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        # 1. Overall Safety Distribution
        fig, ax = plt.subplots(figsize=(10, 6))
        safety_counts = [
            self.report_data['safe_responses'],
            self.report_data['vulnerable_responses'],
            self.report_data['error_responses']
        ]
        safety_labels = ['Safe', 'Vulnerable', 'Error']
        colors = ['green', 'red', 'orange']
        
        bars = ax.bar(safety_labels, safety_counts, color=colors, alpha=0.7)
        ax.set_title('Overall Safety Assessment Distribution', fontsize=16, fontweight='bold')
        ax.set_ylabel('Number of Responses')
        
        # Add value labels on bars
        for bar, count in zip(bars, safety_counts):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                   f'{count}\n({count/self.report_data["total_tests"]*100:.1f}%)',
                   ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(output_path / 'safety_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Vulnerability by Target Type
        if self.report_data['vulnerability_summary']:
            fig, ax = plt.subplots(figsize=(12, 6))
            vuln_types = list(self.report_data['vulnerability_summary'].keys())
            vuln_counts = list(self.report_data['vulnerability_summary'].values())
            
            # Clean up labels
            clean_labels = [label.replace('_', ' ').title() for label in vuln_types]
            
            bars = ax.bar(clean_labels, vuln_counts, alpha=0.7)
            ax.set_title('Vulnerabilities by Target Type', fontsize=16, fontweight='bold')
            ax.set_ylabel('Number of Vulnerabilities')
            ax.tick_params(axis='x', rotation=45)
            
            # Add value labels
            for bar, count in zip(bars, vuln_counts):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                       str(count), ha='center', va='bottom')
            
            plt.tight_layout()
            plt.savefig(output_path / 'vulnerabilities_by_type.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        # 3. Severity Distribution
        if self.report_data['severity_distribution']:
            fig, ax = plt.subplots(figsize=(10, 6))
            severities = sorted(self.report_data['severity_distribution'].keys())
            counts = [self.report_data['severity_distribution'][str(s)] for s in severities]
            severity_labels = ["None", "Low", "Low-Med", "Medium", "High", "Critical"]
            
            bars = ax.bar([severity_labels[int(s)] for s in severities], counts, alpha=0.7)
            ax.set_title('Severity Score Distribution', fontsize=16, fontweight='bold')
            ax.set_ylabel('Number of Responses')
            ax.set_xlabel('Severity Level')
            
            # Add value labels
            for bar, count in zip(bars, counts):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                       str(count), ha='center', va='bottom')
            
            plt.tight_layout()
            plt.savefig(output_path / 'severity_distribution.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        # 4. Model Comparison (if multiple models)
        if len(self.df_results['model_name'].unique()) > 1:
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
            
            # Vulnerability rate by model
            model_vuln_rates = self.df_results.groupby('model_name')['vulnerability_detected'].agg(['count', 'sum'])
            model_vuln_rates['rate'] = model_vuln_rates['sum'] / model_vuln_rates['count'] * 100
            
            bars1 = ax1.bar(model_vuln_rates.index, model_vuln_rates['rate'], alpha=0.7)
            ax1.set_title('Vulnerability Rate by Model')
            ax1.set_ylabel('Vulnerability Rate (%)')
            ax1.tick_params(axis='x', rotation=45)
            
            for bar, rate in zip(bars1, model_vuln_rates['rate']):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{rate:.1f}%', ha='center', va='bottom')
            
            # Average severity by model
            model_severity = self.df_results.groupby('model_name')['severity_score'].mean()
            bars2 = ax2.bar(model_severity.index, model_severity.values, alpha=0.7, color='orange')
            ax2.set_title('Average Severity Score by Model')
            ax2.set_ylabel('Average Severity Score')
            ax2.tick_params(axis='x', rotation=45)
            
            for bar, severity in zip(bars2, model_severity.values):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,
                        f'{severity:.2f}', ha='center', va='bottom')
            
            # Response time by model
            model_time = self.df_results.groupby('model_name')['execution_time'].mean()
            bars3 = ax3.bar(model_time.index, model_time.values, alpha=0.7, color='green')
            ax3.set_title('Average Response Time by Model')
            ax3.set_ylabel('Response Time (seconds)')
            ax3.tick_params(axis='x', rotation=45)
            
            for bar, time_val in zip(bars3, model_time.values):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{time_val:.2f}s', ha='center', va='bottom')
            
            # Vulnerability heatmap by model and template category
            pivot_data = self.df_results.pivot_table(
                values='vulnerability_detected',
                index='model_name',
                columns='template_category',
                aggfunc='mean',
                fill_value=0
            )
            
            sns.heatmap(pivot_data, ax=ax4, annot=True, fmt='.2f', cmap='Reds',
                       cbar_kws={'label': 'Vulnerability Rate'})
            ax4.set_title('Vulnerability Rate Heatmap: Model vs Template Category')
            ax4.set_xlabel('Template Category')
            ax4.set_ylabel('Model')
            
            plt.tight_layout()
            plt.savefig(output_path / 'model_comparison.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        # 5. Template Performance Analysis
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Vulnerability rate by template
        template_stats = self.df_results.groupby('template_id').agg({
            'vulnerability_detected': ['count', 'sum'],
            'severity_score': 'mean'
        })
        template_stats.columns = ['total_tests', 'vulnerable_tests', 'avg_severity']
        template_stats['vuln_rate'] = template_stats['vulnerable_tests'] / template_stats['total_tests'] * 100
        
        # Sort by vulnerability rate
        template_stats_sorted = template_stats.sort_values('vuln_rate', ascending=True)
        
        bars1 = ax1.barh(range(len(template_stats_sorted)), template_stats_sorted['vuln_rate'], alpha=0.7)
        ax1.set_yticks(range(len(template_stats_sorted)))
        ax1.set_yticklabels(template_stats_sorted.index, fontsize=8)
        ax1.set_xlabel('Vulnerability Rate (%)')
        ax1.set_title('Vulnerability Rate by Test Template')
        
        # Add value labels
        for i, (bar, rate) in enumerate(zip(bars1, template_stats_sorted['vuln_rate'])):
            width = bar.get_width()
            ax1.text(width + 1, bar.get_y() + bar.get_height()/2.,
                    f'{rate:.1f}%', ha='left', va='center', fontsize=8)
        
        # Average severity by template
        template_stats_sorted_sev = template_stats.sort_values('avg_severity', ascending=True)
        bars2 = ax2.barh(range(len(template_stats_sorted_sev)), template_stats_sorted_sev['avg_severity'], 
                        alpha=0.7, color='orange')
        ax2.set_yticks(range(len(template_stats_sorted_sev)))
        ax2.set_yticklabels(template_stats_sorted_sev.index, fontsize=8)
        ax2.set_xlabel('Average Severity Score')
        ax2.set_title('Average Severity Score by Test Template')
        
        # Add value labels
        for i, (bar, severity) in enumerate(zip(bars2, template_stats_sorted_sev['avg_severity'])):
            width = bar.get_width()
            ax2.text(width + 0.05, bar.get_y() + bar.get_height()/2.,
                    f'{severity:.2f}', ha='left', va='center', fontsize=8)
        
        plt.tight_layout()
        plt.savefig(output_path / 'template_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"Visualizations saved to {output_path}/")
    
    def export_detailed_csv(self, output_path: str = "detailed_results.csv"):
        """Export detailed results to CSV for further analysis."""
        self.df_results.to_csv(output_path, index=False)
        print(f"Detailed results exported to {output_path}")
    
    def get_high_risk_findings(self, min_severity: int = 4) -> pd.DataFrame:
        """Get high-risk findings that require immediate attention."""
        high_risk = self.df_results[
            (self.df_results['severity_score'] >= min_severity) |
            (self.df_results['vulnerability_detected'] == True)
        ].copy()
        
        return high_risk[['test_id', 'model_name', 'template_title', 'safety_assessment', 
                         'severity_score', 'analysis_notes']].sort_values('severity_score', ascending=False)
    
    def generate_executive_summary(self) -> str:
        """Generate an executive summary for stakeholders."""
        total_tests = self.report_data['total_tests']
        vulnerable_pct = (self.report_data['vulnerable_responses'] / total_tests) * 100
        
        high_risk_count = len(self.df_results[self.df_results['severity_score'] >= 4])
        
        # Find most problematic model
        if len(self.df_results['model_name'].unique()) > 1:
            model_vuln_rates = self.df_results.groupby('model_name')['vulnerability_detected'].agg(['count', 'sum'])
            model_vuln_rates['rate'] = model_vuln_rates['sum'] / model_vuln_rates['count']
            most_vulnerable_model = model_vuln_rates['rate'].idxmax()
            most_vulnerable_rate = model_vuln_rates['rate'].max() * 100
        else:
            most_vulnerable_model = self.df_results['model_name'].iloc[0]
            most_vulnerable_rate = vulnerable_pct
        
        # Find most problematic vulnerability type
        if self.report_data['vulnerability_summary']:
            most_common_vuln = max(self.report_data['vulnerability_summary'], 
                                 key=self.report_data['vulnerability_summary'].get)
            most_common_vuln_count = self.report_data['vulnerability_summary'][most_common_vuln]
        else:
            most_common_vuln = "None identified"
            most_common_vuln_count = 0
        
        summary = f"""
EXECUTIVE SUMMARY: UPOF RED TEAM EVALUATION

OVERVIEW:
- Total tests conducted: {total_tests}
- Overall vulnerability rate: {vulnerable_pct:.1f}%
- High-risk findings: {high_risk_count}
- Models tested: {len(self.df_results['model_name'].unique())}

KEY FINDINGS:
- Most vulnerable model: {most_vulnerable_model} ({most_vulnerable_rate:.1f}% vulnerability rate)
- Primary vulnerability type: {most_common_vuln.replace('_', ' ').title()} ({most_common_vuln_count} instances)
- Critical issues requiring immediate attention: {high_risk_count}

RISK ASSESSMENT:
{"HIGH RISK" if vulnerable_pct > 20 else "MEDIUM RISK" if vulnerable_pct > 10 else "LOW RISK"} - {vulnerable_pct:.1f}% of tests revealed vulnerabilities

RECOMMENDATIONS:
"""
        
        for i, rec in enumerate(self.report_data['recommendations'], 1):
            summary += f"{i}. {rec}\n"
        
        return summary

def main():
    """Command-line interface for the report viewer."""
    parser = argparse.ArgumentParser(description='UPOF Red Team Report Viewer')
    parser.add_argument('report_path', help='Path to the evaluation report JSON file')
    parser.add_argument('--summary', action='store_true', help='Print summary only')
    parser.add_argument('--visualize', action='store_true', help='Generate visualizations')
    parser.add_argument('--export-csv', action='store_true', help='Export detailed results to CSV')
    parser.add_argument('--high-risk', action='store_true', help='Show high-risk findings only')
    parser.add_argument('--executive-summary', action='store_true', help='Generate executive summary')
    parser.add_argument('--output-dir', default='output', help='Output directory for generated files')
    
    args = parser.parse_args()
    
    # Initialize viewer
    viewer = UPOFReportViewer(args.report_path)
    
    if args.summary or not any([args.visualize, args.export_csv, args.high_risk, args.executive_summary]):
        viewer.print_summary()
    
    if args.visualize:
        viewer.create_visualizations(args.output_dir)
    
    if args.export_csv:
        csv_path = Path(args.output_dir) / 'detailed_results.csv'
        Path(args.output_dir).mkdir(exist_ok=True)
        viewer.export_detailed_csv(str(csv_path))
    
    if args.high_risk:
        high_risk_findings = viewer.get_high_risk_findings()
        print("\nHIGH-RISK FINDINGS:")
        print("=" * 50)
        if len(high_risk_findings) > 0:
            for _, finding in high_risk_findings.iterrows():
                print(f"\nTest ID: {finding['test_id']}")
                print(f"Model: {finding['model_name']}")
                print(f"Template: {finding['template_title']}")
                print(f"Safety Assessment: {finding['safety_assessment']}")
                print(f"Severity Score: {finding['severity_score']}")
                print(f"Analysis: {finding['analysis_notes']}")
                print("-" * 30)
        else:
            print("No high-risk findings detected.")
    
    if args.executive_summary:
        summary = viewer.generate_executive_summary()
        print(summary)
        
        # Save to file
        summary_path = Path(args.output_dir) / 'executive_summary.txt'
        Path(args.output_dir).mkdir(exist_ok=True)
        with open(summary_path, 'w') as f:
            f.write(summary)
        print(f"\nExecutive summary saved to {summary_path}")

if __name__ == "__main__":
    main()